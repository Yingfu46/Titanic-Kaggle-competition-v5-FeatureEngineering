{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-04T07:37:55.719762Z","iopub.execute_input":"2022-08-04T07:37:55.720223Z","iopub.status.idle":"2022-08-04T07:37:55.731233Z","shell.execute_reply.started":"2022-08-04T07:37:55.720184Z","shell.execute_reply":"2022-08-04T07:37:55.730324Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"**Get the data**","metadata":{}},{"cell_type":"code","source":"## Titanic competition in Kaggle\n## data downloaded from Kaggle.\nfrom ast import Not\nfrom statistics import mean\nfrom matplotlib import pyplot as plt\nimport pandas\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport seaborn as sns\n\n\ntrain = pandas.read_csv('/kaggle/input/titanic/train.csv')\ntrain.columns  ","metadata":{"execution":{"iopub.status.busy":"2022-08-04T07:38:01.653128Z","iopub.execute_input":"2022-08-04T07:38:01.653627Z","iopub.status.idle":"2022-08-04T07:38:02.532454Z","shell.execute_reply.started":"2022-08-04T07:38:01.653582Z","shell.execute_reply":"2022-08-04T07:38:02.530677Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# 1. EDA\n\n**1.1** Check the data, including graphs and correlations, both numerical and categorical.","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-04T07:38:05.703392Z","iopub.execute_input":"2022-08-04T07:38:05.703884Z","iopub.status.idle":"2022-08-04T07:38:05.731735Z","shell.execute_reply.started":"2022-08-04T07:38:05.703847Z","shell.execute_reply":"2022-08-04T07:38:05.730677Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"The *Name* variable is not informative. Change to surname (to indicate family tie) \nand title (to indicate marriage and age) ","metadata":{}},{"cell_type":"code","source":"surname = []\ntitle = []\nfor txt in train['Name']:\n      txt1 = txt.split(',')[0]\n      txt2 = (txt.split(\",\")[1]).split(\".\")[0]\n      surname.append(txt1)\n      title.append(txt2)\n\ntrain['Surname'] = pandas.DataFrame(surname)\ntrain['Title'] = pandas.DataFrame(title, dtype='category')\n\n","metadata":{"execution":{"iopub.status.busy":"2022-08-04T07:38:08.113207Z","iopub.execute_input":"2022-08-04T07:38:08.114531Z","iopub.status.idle":"2022-08-04T07:38:08.136232Z","shell.execute_reply.started":"2022-08-04T07:38:08.114481Z","shell.execute_reply":"2022-08-04T07:38:08.134469Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"**An illustraion of data**","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nsns.pairplot(train, hue='Survived')","metadata":{"execution":{"iopub.status.busy":"2022-08-04T07:38:12.626920Z","iopub.execute_input":"2022-08-04T07:38:12.627625Z","iopub.status.idle":"2022-08-04T07:38:23.338192Z","shell.execute_reply.started":"2022-08-04T07:38:12.627584Z","shell.execute_reply":"2022-08-04T07:38:23.337029Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"From the graph above, among all numeric variables, Pclass 1, minor age, \n2 SibSp, and extreme high fare seem to have high survival probability.\n","metadata":{}},{"cell_type":"code","source":"corr_matrix = train.corr()\nprint(corr_matrix['Survived'].sort_values(ascending=False))\n","metadata":{"execution":{"iopub.status.busy":"2022-08-04T07:38:35.204386Z","iopub.execute_input":"2022-08-04T07:38:35.204806Z","iopub.status.idle":"2022-08-04T07:38:35.216408Z","shell.execute_reply.started":"2022-08-04T07:38:35.204771Z","shell.execute_reply":"2022-08-04T07:38:35.214608Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"It looks that all numeric variables except PassengerId are correlated to Survived.\nCheck some categorical variables too.","metadata":{}},{"cell_type":"code","source":"sns.histplot(data=train, x='Sex',hue='Survived')","metadata":{"execution":{"iopub.status.busy":"2022-08-03T19:15:51.144832Z","iopub.execute_input":"2022-08-03T19:15:51.145268Z","iopub.status.idle":"2022-08-03T19:15:51.350212Z","shell.execute_reply.started":"2022-08-03T19:15:51.145232Z","shell.execute_reply":"2022-08-03T19:15:51.349091Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"sns.histplot(data=train, x='Title',hue='Survived')","metadata":{"execution":{"iopub.status.busy":"2022-08-03T19:15:53.526351Z","iopub.execute_input":"2022-08-03T19:15:53.527264Z","iopub.status.idle":"2022-08-03T19:15:53.861295Z","shell.execute_reply.started":"2022-08-03T19:15:53.527218Z","shell.execute_reply":"2022-08-03T19:15:53.860040Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"**Too many kind of Titles**. Decrease.","metadata":{}},{"cell_type":"code","source":"train.loc[:,'Title'].value_counts()\n\ntitle_list = ((train.loc[:,'Title'].value_counts())[:6]).index.to_list()\n\nfor ind in train.index:\n    if not(train.loc[ind, 'Title'] in title_list):\n        train.loc[ind, 'Title'] = \"Others\"\n\ntrain.loc[:,'Title'].value_counts()\n\nsns.histplot(data=train, x='Title',hue='Survived')","metadata":{"execution":{"iopub.status.busy":"2022-08-04T07:46:57.547643Z","iopub.execute_input":"2022-08-04T07:46:57.548106Z","iopub.status.idle":"2022-08-04T07:46:57.858965Z","shell.execute_reply.started":"2022-08-04T07:46:57.548070Z","shell.execute_reply":"2022-08-04T07:46:57.857337Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"***Title* looks much better now.**","metadata":{}},{"cell_type":"markdown","source":"**1.2 Missing values**","metadata":{}},{"cell_type":"code","source":"# missing values in train\nnp.sum(train.isna(), axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-08-04T07:47:07.739210Z","iopub.execute_input":"2022-08-04T07:47:07.739721Z","iopub.status.idle":"2022-08-04T07:47:07.751819Z","shell.execute_reply.started":"2022-08-04T07:47:07.739682Z","shell.execute_reply":"2022-08-04T07:47:07.750899Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"We split the train data to own train and test set \nbefore treatment of missing values, in order not to overfit.\nVariables *Name* and *PassengerID* is droped.","metadata":{}},{"cell_type":"code","source":"X = train.drop(['Survived','Name', 'PassengerId'],axis=1)\ny = (train.copy())['Survived']\n\n# Split the (training) data to train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=4686)\n\n# missing values in X_train\nnp.sum(X_train.isna(), axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-08-04T07:47:16.536770Z","iopub.execute_input":"2022-08-04T07:47:16.537190Z","iopub.status.idle":"2022-08-04T07:47:16.555685Z","shell.execute_reply.started":"2022-08-04T07:47:16.537156Z","shell.execute_reply":"2022-08-04T07:47:16.554160Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Fix missing values with means of corresponding group. Use the powerful *groupby* in *pandas*.\n**age => Title and Sex**\n\n**Embarked => Pclass and Sex**","metadata":{}},{"cell_type":"code","source":"X_train[\"Age\"] = X_train[\"Age\"].fillna(X_train.groupby(['Sex','Title'])['Age'].transform('mean'))\nX_test[\"Age\"] = X_test[\"Age\"].fillna(X_test.groupby(['Sex','Title'])['Age'].transform('mean'))\n\n# There is no suitable function of \"Most-frequent\" for 'transform'.\n\nfill_values = ((X_train.groupby(['Pclass','Sex'])['Embarked']).describe())['top']\n\nfor ind in (X_train.loc[X_train['Embarked'].isna()]).index:\n    X_train.loc[ind, 'Embarked'] = fill_values[(X_train.loc[ind,'Pclass'], X_train.loc[ind,'Sex'])]\n","metadata":{"execution":{"iopub.status.busy":"2022-08-04T07:47:32.118426Z","iopub.execute_input":"2022-08-04T07:47:32.118806Z","iopub.status.idle":"2022-08-04T07:47:32.158274Z","shell.execute_reply.started":"2022-08-04T07:47:32.118774Z","shell.execute_reply":"2022-08-04T07:47:32.157338Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"There are too many missing values in \"Cabin\" with uncertain impact \nto the accuracy after imputation. (It seems improve a little bit \nafter evaluation.)","metadata":{}},{"cell_type":"markdown","source":"# 2. Preprocessing\nSimple imputation for Cabin, Encode and Pipeline\nImport packages from SciKit-Learn","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.preprocessing import OneHotEncoder, PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import cross_val_score, RandomizedSearchCV, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.inspection import permutation_importance","metadata":{"execution":{"iopub.status.busy":"2022-08-04T07:47:41.917929Z","iopub.execute_input":"2022-08-04T07:47:41.918366Z","iopub.status.idle":"2022-08-04T07:47:42.214043Z","shell.execute_reply.started":"2022-08-04T07:47:41.918333Z","shell.execute_reply":"2022-08-04T07:47:42.212629Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"**Set up preprocessing with Pipeline (ColumnTransformer)**","metadata":{}},{"cell_type":"code","source":"Impute1 = SimpleImputer(strategy='constant')\nImpute2 = SimpleImputer(strategy=\"most_frequent\")\nEncode = OneHotEncoder(handle_unknown=\"ignore\")\nScale = StandardScaler()\n\nNum_attr1 = ['Fare']\nNum_attr2 = ['Age']\nAttrib2 = ['Pclass', 'Sex', 'Ticket', 'Cabin', 'Embarked', 'Surname', 'Title']\n\n#Attrib1 = X_train.select_dtypes(include = np.number).columns.to_list()\n#Attrib2 = X_train.select_dtypes(exclude = np.number).columns.to_list()\n\nnum_pipiline1 = Pipeline(steps=[\n      ('poly', PolynomialFeatures()),\n    ('scale1', StandardScaler())\n])\n\nnum_pipiline2 = Pipeline(steps=[\n    ('scale1', StandardScaler())\n])\n\ncat_pipeline1 = Pipeline(steps=[\n    ('enco', Encode)\n])\n\ntotal_pipeline = ColumnTransformer(transformers=[\n    ('num1', num_pipiline1, Num_attr1),\n    ('num2', num_pipiline2, Num_attr2),\n    ('cat', cat_pipeline1, Attrib2)\n], remainder=\"passthrough\")\n","metadata":{"execution":{"iopub.status.busy":"2022-08-04T07:47:49.048814Z","iopub.execute_input":"2022-08-04T07:47:49.049334Z","iopub.status.idle":"2022-08-04T07:47:49.061069Z","shell.execute_reply.started":"2022-08-04T07:47:49.049291Z","shell.execute_reply":"2022-08-04T07:47:49.059161Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# 3. Logistic regression\nLogistic regression is the most natural choice for this competition.","metadata":{}},{"cell_type":"code","source":"Logit = Pipeline(steps=[ \n    ('preprocess', total_pipeline),\n    ('logit', LogisticRegression(penalty='l2', C=1, max_iter=1000, solver=\"liblinear\") )\n]\n)\n\ntitanic_logit = Logit.fit(X_train, y_train)\ntitanic_pred = Logit.predict(X_train)\nprint(classification_report(y_train, titanic_pred))\n# accuracy 0.94","metadata":{"execution":{"iopub.status.busy":"2022-08-04T07:47:55.344470Z","iopub.execute_input":"2022-08-04T07:47:55.344888Z","iopub.status.idle":"2022-08-04T07:47:55.410352Z","shell.execute_reply.started":"2022-08-04T07:47:55.344855Z","shell.execute_reply":"2022-08-04T07:47:55.408700Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"It is not bad. However, only for the train data.","metadata":{}},{"cell_type":"code","source":"cross_logit = cross_val_score(Logit, X_train, y_train, cv=10)\nprint(\"Cross_val_score is \", np.mean(cross_logit))\n# ~ 0.8428312980551788\nprint(\"For own created test dataset\")\nprint(classification_report(y_test, Logit.predict(X_test)))\n# 0.85","metadata":{"execution":{"iopub.status.busy":"2022-08-04T07:48:12.674554Z","iopub.execute_input":"2022-08-04T07:48:12.674999Z","iopub.status.idle":"2022-08-04T07:48:13.163231Z","shell.execute_reply.started":"2022-08-04T07:48:12.674966Z","shell.execute_reply":"2022-08-04T07:48:13.161544Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"It is a rather good accuracy (look at the leaderboard). We can use GridSearch to tune parameters. It is not run to save running time. One option is as follows.\n\ndist = {\n      'preprocess__num1__poly__degree': range(1,4),\n    'logit__C': range(0,1000,50), \n    'logit__penalty': ['l2','l1']\n}\n\nlr_clf = RandomizedSearchCV(Logit, param_distributions=dist, n_iter=10, cv=3)\n\nlr_clf.fit(X_train, y_train)\n\nprint(classification_report(y_train, lr_clf.predict(X_train)))\n#1 \n\n'''\nbest_params_:\n'preprocess__num1__poly__degree': 3, 'logit__penalty': 'l2', 'logit__C': 900\n'''\n\nprint(np.mean(cross_val_score(lr_clf, X_train, y_train, cv=10)))\n#0.842740841248304  Overfitting.\n\nprint(classification_report(y_test, lr_clf.predict(X_test)))\n#~0.83\n","metadata":{}},{"cell_type":"markdown","source":"It is not better than the default model. We can try *SDGClassifier*.","metadata":{}},{"cell_type":"code","source":"Logit2 = Pipeline(steps=[ \n    ('preprocess', total_pipeline),\n    ('logit2', SGDClassifier(penalty='l2', alpha=0.0001, max_iter=1000, loss=\"log\") )\n]\n)\ntitanic_logit = Logit2.fit(X_train, y_train)\nprint(\"Accuracy train: \", classification_report(y_train, Logit2.predict(X_train)))\n# 1\nprint(\"Cross_val_score is \", np.mean(cross_val_score(Logit2, X_train, y_train, cv=10)))\n# ~0.83\nprint(\"Accuracy for own test data:\")\nprint(classification_report(y_test, Logit2.predict(X_test)))\n# 0.78","metadata":{"execution":{"iopub.status.busy":"2022-08-04T07:49:04.858737Z","iopub.execute_input":"2022-08-04T07:49:04.859173Z","iopub.status.idle":"2022-08-04T07:49:05.370228Z","shell.execute_reply.started":"2022-08-04T07:49:04.859138Z","shell.execute_reply":"2022-08-04T07:49:05.368719Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"Not better either. \n\n**Check Feature permutation importance to select feature**","metadata":{}},{"cell_type":"code","source":"fpi = permutation_importance(estimator=Logit, X= X_train, y= y_train, n_repeats=8, random_state=4686)\n\nsns.barplot(\n    data=pandas.DataFrame({\n        'permutation importance': fpi['importances_mean'],\n        'feature': X_train.columns\n    }).sort_values('permutation importance', ascending=False),\n    y='feature', x='permutation importance'\n)\nplt.title('Logistic: feature permutation importance on train data')\n","metadata":{"execution":{"iopub.status.busy":"2022-08-04T07:49:51.985769Z","iopub.execute_input":"2022-08-04T07:49:51.986215Z","iopub.status.idle":"2022-08-04T07:49:54.086850Z","shell.execute_reply.started":"2022-08-04T07:49:51.986180Z","shell.execute_reply":"2022-08-04T07:49:54.085122Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"The graph shows that Embarked, Fare (surprisingly), Parch, SibSp, and Cabin\nhave low permutation importance. Try to leave them.","metadata":{}},{"cell_type":"code","source":"Num_attr2 = ['Age']\nCat_logit = ['Pclass', 'Sex', 'Ticket', 'Surname', 'Title']\n\ntotal_lg = ColumnTransformer(transformers=[\n    ('num2', num_pipiline2, Num_attr2),\n    ('cat', cat_pipeline1, Cat_logit)\n], remainder=\"drop\")\n\nLogit3 = Pipeline(steps=[ \n    ('preprocess', total_lg),\n    ('logit', LogisticRegression(penalty='l2', C=1000, max_iter=1000, solver=\"liblinear\") )\n]\n)\n\nLogit3.fit(X_train, y_train)\nprint(\"Accuracy for the train data:\")\nprint(classification_report(y_train, Logit3.predict(X_train)))\n# 1\n\nprint(\"Cross_val_score: \", np.mean(cross_val_score(Logit3, X_train, y_train, cv=10)))\n# ~0.84\nprint(\"Accuracy for the test data set:\\n\")\nprint(classification_report(y_test, Logit3.predict(X_test)))\n# 0.80\n\n","metadata":{"execution":{"iopub.status.busy":"2022-08-04T07:50:19.888891Z","iopub.execute_input":"2022-08-04T07:50:19.889333Z","iopub.status.idle":"2022-08-04T07:50:20.270070Z","shell.execute_reply.started":"2022-08-04T07:50:19.889298Z","shell.execute_reply":"2022-08-04T07:50:20.268462Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"And run GridSearch again. \n\n**It is no problem with error messages (*some_fits_failed_message*) below during some of Grid Searches.**","metadata":{}},{"cell_type":"code","source":"dist3 = {\n    'logit__C': range(0,1000,50), \n    'logit__penalty': ['l2','l1']\n}\n\nLogit3_chosen = RandomizedSearchCV(Logit3, param_distributions=dist3, n_iter=10, cv=5)\nLogit3_chosen.fit(X_train, y_train)\nprint(\"Accuracy for the train data:\")\nprint(classification_report(y_train, Logit3_chosen.predict(X_train)))\n#1\nprint(\"Cross_val_score is: \", np.mean(cross_val_score(Logit3_chosen, X_train, y_train, cv=10)))\n# ~0.84\nprint(\"Accuracy for the test data:\")\nprint(classification_report(y_test, Logit3_chosen.predict(X_test)))\n# 0.85","metadata":{"execution":{"iopub.status.busy":"2022-08-04T07:50:44.312470Z","iopub.execute_input":"2022-08-04T07:50:44.312895Z","iopub.status.idle":"2022-08-04T07:51:02.028815Z","shell.execute_reply.started":"2022-08-04T07:50:44.312860Z","shell.execute_reply":"2022-08-04T07:51:02.027475Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# 3. Support Vector Machine","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\n\nsvc = Pipeline(steps=[ \n    ('preprocess', total_pipeline),\n    ('svm', SVC(C=100, kernel=\"linear\"))\n])\n\ntitan_svc = svc.fit(X_train, y_train)\nprint(\"SVM Accuracy for the train data:\")\nprint(classification_report(y_train, svc.predict(X_train)))\n##  1.  \nprint(\"Cross_val_score: \", np.mean(cross_val_score(svc, X_train, y_train, cv=5)))\n# ~0.84\nprint(\"SVM Accuracy for own test data:\")\nprint(classification_report(y_test, svc.predict(X_test)))\n## 0.83\n","metadata":{"execution":{"iopub.status.busy":"2022-08-04T07:58:32.543259Z","iopub.execute_input":"2022-08-04T07:58:32.543719Z","iopub.status.idle":"2022-08-04T07:58:33.024361Z","shell.execute_reply.started":"2022-08-04T07:58:32.543683Z","shell.execute_reply":"2022-08-04T07:58:33.022884Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"To run GridSearch to tune paremeters:\n\ndist_svc= { \n    \"svm__C\": range(1, 1000, 100),\n    \"svm__kernel\": ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed']\n}\n\nsvc_chosen = RandomizedSearchCV(svc, param_distributions=dist_svc, n_iter=50, random_state=4686)\nsvc_chosen.fit(X_train, y_train)\n\n#svc_chosen.best_params_\n#{'svm__kernel': 'linear', 'svm__C': 1}\n#It takes about one hour to run. n*m*log_2(m)\n\nWe use direct the best parameters.","metadata":{}},{"cell_type":"code","source":"svc_chosen = Pipeline(steps=[ \n    ('preprocess', total_pipeline),\n    ('svm', SVC(C=1, kernel=\"linear\"))\n])\n\nsvc_chosen.fit(X_train, y_train)\npred_svc_rs = svc_chosen.predict(X_train)\nprint(\"SVM Accuracy for the train data:\")\nprint(classification_report(y_train, pred_svc_rs))\n# 1 \nprint(\"Cross_val_score: \", np.mean(cross_val_score(svc_chosen, X_train, y_train, cv=10)))\n# 0.8487788331071913\nprint(\"SVM Accuracy for test data:\")\nprint(classification_report(y_test, svc_chosen.predict(X_test)))\n#0.84","metadata":{"execution":{"iopub.status.busy":"2022-08-04T07:58:51.014092Z","iopub.execute_input":"2022-08-04T07:58:51.014600Z","iopub.status.idle":"2022-08-04T07:58:51.945145Z","shell.execute_reply.started":"2022-08-04T07:58:51.014561Z","shell.execute_reply":"2022-08-04T07:58:51.943322Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"**3.1 Permutation Importance**\n\nSee https://scikit-learn.org/stable/modules/permutation_importance.html for referee on feature permutation importance.","metadata":{}},{"cell_type":"code","source":"fpi_svm = permutation_importance(estimator=svc_chosen, X= X_train, y= y_train, n_repeats=8, random_state=4686)\n\nsns.barplot(\n    data=pandas.DataFrame({\n        'permutation importance': fpi_svm['importances_mean'],\n        'feature': X_train.columns\n    }).sort_values('permutation importance', ascending=False),\n    y='feature', x='permutation importance'\n)\nplt.title('SVM Feature permutation importance on X_train')","metadata":{"execution":{"iopub.status.busy":"2022-08-04T07:59:04.021406Z","iopub.execute_input":"2022-08-04T07:59:04.021862Z","iopub.status.idle":"2022-08-04T07:59:07.413681Z","shell.execute_reply.started":"2022-08-04T07:59:04.021826Z","shell.execute_reply":"2022-08-04T07:59:07.412124Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"*Embarked* and *Parch* have low importance. Surprisingly, *Pclass, Fare, Age* are not high either. It comfuses somehow. We leave *Embarked* and *Parch*, but not others.","metadata":{}},{"cell_type":"code","source":"num1_svm = Num_attr1\nnum2_svm = Num_attr2\ncat1 = ['Sex', 'Ticket', 'Surname', 'Title', 'Cabin', 'Pclass']\n\ntotal_svm = ColumnTransformer(transformers=[ \n    ('num1', num_pipiline1, num1_svm),\n    ('num2', num_pipiline2, num2_svm),\n    ('cat1', cat_pipeline1, cat1)\n], remainder=\"drop\")\n\nsvc2 = Pipeline(steps=[ \n    ('prep', total_svm),\n    ('svc2',SVC(C=1, kernel=\"linear\"))\n])\n\nsvc2.fit(X_train, y_train)\nprint(\"SVC 2 accuracy for train data: \")\nprint(classification_report(y_train, svc2.predict(X_train)))\n#1\nprint(np.mean(cross_val_score(svc2, X_train, y_train, cv=10)))\n# 0.85\nprint(\"SVC 2 accuracy for test data: \")\nprint(classification_report(y_test, svc2.predict(X_test)))\n# 0.86\n","metadata":{"execution":{"iopub.status.busy":"2022-08-04T08:00:00.420455Z","iopub.execute_input":"2022-08-04T08:00:00.420880Z","iopub.status.idle":"2022-08-04T08:00:01.184986Z","shell.execute_reply.started":"2022-08-04T08:00:00.420844Z","shell.execute_reply":"2022-08-04T08:00:01.183779Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"**A litte improvement after feature selection.**","metadata":{}},{"cell_type":"markdown","source":"# 4. Random Forest\n\nWe repeat the process as for Random Forest. Cross Validation, GridSearch, feature selection, and so on.","metadata":{}},{"cell_type":"code","source":"random_forest = Pipeline(steps=[ \n    ('preprocess', total_pipeline),\n    ('rf', RandomForestClassifier(criterion=\"entropy\", min_samples_leaf=1, random_state=4686))\n])\n\ntitan_rf = random_forest.fit(X_train, y_train)\nprint(\"Random Forest Accuracy for train data:\")\nprint(classification_report(y_train, random_forest.predict(X_train)))\n#  1\nprint(\"Cross_val_score: \", np.mean(cross_val_score(random_forest, X_train, y_train, cv=10)))\n# ~0.83 \n\nprint(\"Random Forest Accuracy for own test data:\")\nprint(classification_report(y_test, random_forest.predict(X_test)))\n# 0.86","metadata":{"execution":{"iopub.status.busy":"2022-08-04T08:01:35.303609Z","iopub.execute_input":"2022-08-04T08:01:35.304100Z","iopub.status.idle":"2022-08-04T08:01:39.159512Z","shell.execute_reply.started":"2022-08-04T08:01:35.304061Z","shell.execute_reply":"2022-08-04T08:01:39.157921Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"Grid Search: \n\ndist_rf = {\n      'preprocess__num1__poly__degree': range(2,4),\n      'rf__criterion': ['gini','entropy'],\n      'rf__max_depth': np.arange(5, 10),\n      'rf__min_samples_leaf': np.arange(1, 15, 2)\n}\n\nrf_chosen = GridSearchCV(random_forest, param_grid=dist_rf, cv=10)\nrf_chosen.fit(X_train, y_train)\nprint(classification_report(y_train, rf_chosen.predict(X_train)))\n#0.86\n\n'''\n{'preprocess__num1__poly__degree': 2,\n 'rf__criterion': 'gini',\n 'rf__max_depth': 9,\n 'rf__min_samples_leaf': 1}\n'''\n\nprint(classification_report(y_test, rf_chosen.predict(X_test)))\n#0.80 \n\nNot as good as the default model above; thus not used in the end. \n","metadata":{}},{"cell_type":"markdown","source":"**Mean Decrease Impurity**\n\nAnother way to check feature importance for Decision trees.","metadata":{}},{"cell_type":"code","source":"feature_rf_num1 = random_forest['preprocess'].named_transformers_['num1'].named_steps['poly'].get_feature_names_out()\nfeature_rf_num2 = random_forest['preprocess'].named_transformers_['num2'].get_feature_names_out()\nfeature_rf_cat1 = random_forest['preprocess'].named_transformers_['cat'].get_feature_names_out()\nfeature_rf_re = X_train.columns[random_forest['preprocess'].transformers_[-1][-1]]\n\nfeature_rf = np.r_[feature_rf_num1, feature_rf_num2, feature_rf_cat1, feature_rf_re]\n\nmdi = random_forest.named_steps['rf'].feature_importances_\n\nsns.barplot(\n    data=pandas.DataFrame({\n        'mean decrease impurity': mdi,\n        'feature': feature_rf\n    }).sort_values('mean decrease impurity', ascending=False).iloc[0:11],\n    y='feature', x='mean decrease impurity'\n)\nplt.title('Random Forest: mean decrease impurity on X_train')","metadata":{"execution":{"iopub.status.busy":"2022-08-04T08:02:25.168351Z","iopub.execute_input":"2022-08-04T08:02:25.168846Z","iopub.status.idle":"2022-08-04T08:02:25.445691Z","shell.execute_reply.started":"2022-08-04T08:02:25.168807Z","shell.execute_reply":"2022-08-04T08:02:25.444769Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"MDI for all variables are not so high. Check Permutation importances too.","metadata":{}},{"cell_type":"code","source":"fpi_rf = permutation_importance(estimator=random_forest, X= X_train, y= y_train, n_repeats=8, random_state=4686)\n\nfpi_rf_df = pandas.DataFrame({\n        'permutation importance': fpi_rf['importances_mean'],\n        'feature': X_train.columns\n    }).sort_values('permutation importance', ascending=False)\n\nprint(fpi_rf_df)\n\nsns.barplot(data=fpi_rf_df, y='feature', x='permutation importance')\n\nplt.title('permutation importance Random Forest on X_train')\n","metadata":{"execution":{"iopub.status.busy":"2022-08-04T08:14:44.040639Z","iopub.execute_input":"2022-08-04T08:14:44.041220Z","iopub.status.idle":"2022-08-04T08:14:48.745269Z","shell.execute_reply.started":"2022-08-04T08:14:44.041172Z","shell.execute_reply":"2022-08-04T08:14:48.743617Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"Permutation importances are low for some variables. We test to leave alone *SibSp* and *Parch*.","metadata":{}},{"cell_type":"code","source":"rf_chosen2 = Pipeline(steps=[ \n    ('prep2', total_svm), #same variables as SVM\n    ('rf2', RandomForestClassifier(criterion=\"entropy\",max_depth=9,  min_samples_leaf=1, random_state=4686)) \n    #max_depth from the Grid Search above.\n])\n\nrf_chosen2.fit(X_train, y_train)\nprint(\"New Random Forest Accuracy for train data:\")\nprint(classification_report(y_train, rf_chosen2.predict(X_train)))\n# 0.86\nprint('Cross_val_Score is: ', np.mean(cross_val_score(rf_chosen2, X_train, y_train, cv=8)))\n# 0.7994298623063684\nprint(\"New Random Forest Accuracy for own test data:\")\nprint(classification_report(y_test, rf_chosen2.predict(X_test)))\n# 0.77 Not better than before.\n","metadata":{"execution":{"iopub.status.busy":"2022-08-04T08:41:37.331216Z","iopub.execute_input":"2022-08-04T08:41:37.331635Z","iopub.status.idle":"2022-08-04T08:41:39.434635Z","shell.execute_reply.started":"2022-08-04T08:41:37.331597Z","shell.execute_reply":"2022-08-04T08:41:39.432994Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"# 5. K-Neighbors classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nknc = Pipeline(steps=[ \n    ('preprocess', total_pipeline),\n    ('knclf', KNeighborsClassifier())\n])\n\nknc.fit(X_train, y_train)\nprint(\"K-neighbors classifier Accuracy for train data: \")\nprint(classification_report(y_train, knc.predict(X_train)))\n#0.87\nprint()\nprint('Cross_val_score is: ', np.mean(cross_val_score(knc, X_train, y_train)))\n# ~0.81\nprint(\"K-neighbors classifier Accuracy for own test data: \")\nprint(classification_report(y_test, knc.predict(X_test)))\n# 0.80\n","metadata":{"execution":{"iopub.status.busy":"2022-08-04T08:44:17.512710Z","iopub.execute_input":"2022-08-04T08:44:17.513148Z","iopub.status.idle":"2022-08-04T08:44:17.951651Z","shell.execute_reply.started":"2022-08-04T08:44:17.513113Z","shell.execute_reply":"2022-08-04T08:44:17.950190Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"**Tune parameters.**\n","metadata":{}},{"cell_type":"code","source":"disc_knc = {\n     'preprocess__num1__poly__degree': range(2,4),\n    \"knclf__n_neighbors\": range(0,100,5),\n    \"knclf__weights\":['uniform','distance']\n}\n\nknc_grid = RandomizedSearchCV(knc, param_distributions=disc_knc, cv=10, random_state=4686)\nknc_grid.fit(X_train, y_train)\n\nprint('K-neighbors classifier Accuracy for train data:')\nprint(classification_report(y_train, knc_grid.predict(X_train)))\n# 1\nprint()\nprint('Cross_val_score: ', np.mean(cross_val_score(knc_grid, X_train, y_train, cv=10), 0))\n# ~ 0.8. \nprint('K-neighbors classifier Accuracy for own test data:')\nprint(classification_report(y_test, knc_grid.predict(X_test)))\n#0.81\n","metadata":{"execution":{"iopub.status.busy":"2022-08-04T08:53:55.378609Z","iopub.execute_input":"2022-08-04T08:53:55.379068Z","iopub.status.idle":"2022-08-04T08:54:48.231768Z","shell.execute_reply.started":"2022-08-04T08:53:55.379032Z","shell.execute_reply":"2022-08-04T08:54:48.229925Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# Feature permutation importances\n\nfpi_knc = permutation_importance(estimator=knc_grid, X= X_train, y= y_train, n_repeats=8, random_state=4686)\n\nfpi_knc_df = pandas.DataFrame({\n        'permutation importance': fpi_knc['importances_mean'],\n        'feature': X_train.columns\n    }).sort_values('permutation importance', ascending=False)\n\n\nsns.barplot(data=fpi_knc_df, y='feature', x='permutation importance')\nplt.title('permutation importance K-Neighbors Classifier on X_train')\nprint(fpi_knc_df)\n","metadata":{"execution":{"iopub.status.busy":"2022-08-04T08:56:54.569329Z","iopub.execute_input":"2022-08-04T08:56:54.569850Z","iopub.status.idle":"2022-08-04T08:56:59.307064Z","shell.execute_reply.started":"2022-08-04T08:56:54.569811Z","shell.execute_reply":"2022-08-04T08:56:59.305109Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"There are no significant differences of permutation importances.","metadata":{}},{"cell_type":"markdown","source":"# 6. Ensemble algorithm\n\nTry ensemble algorithms such as Voting Classifier.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\n\n# Equal weights\nvote_clf = VotingClassifier(\n    estimators= [('lr', Logit3_chosen), ('knc', knc_grid), ('svc', svc2), ('rf', random_forest)],\n    voting='hard'\n)\n\nvote_clf.fit(X_train, y_train)\nprint('Voting classifier Accuracy for train data: ')\nprint(classification_report(y_train, vote_clf.predict(X_train)))\n# 1\nprint()\nprint('Cross_val_score: ', np.mean(cross_val_score(vote_clf, X_train, y_train, cv=10), 0))\n# 0.8427634554500226\nprint()\nprint('Voting classifier Accuracy for own test data: ')\nprint(classification_report(y_test, vote_clf.predict(X_test)))\n#0.86\n","metadata":{"execution":{"iopub.status.busy":"2022-08-04T09:01:11.618400Z","iopub.execute_input":"2022-08-04T09:01:11.618845Z","iopub.status.idle":"2022-08-04T09:02:25.583237Z","shell.execute_reply.started":"2022-08-04T09:01:11.618806Z","shell.execute_reply":"2022-08-04T09:02:25.581924Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"*Soft*-voting is unavailable since some of the algorithms don't have *predict_proba*. \nWe try to skip the K-neighbors classifier since it is the worst one among others.","metadata":{}},{"cell_type":"code","source":"vote_clf2 = VotingClassifier(\n    estimators= [('lr', Logit3_chosen), ('svc', svc2), ('rf', random_forest)],\n    voting='hard'\n)\n\nvote_clf2.fit(X_train, y_train)\nprint(\"Voting classifier 2 Accuracy for train data: \")\nprint(classification_report(y_train, vote_clf2.predict(X_train)))\n# 1\nprint()\nprint('Cross_val_score: ', np.mean(cross_val_score(vote_clf2, X_train, y_train, cv=10), 0))\n# ~0.84\nprint()\nprint(\"Voting classifier 2 Accuracy for test data: \")\nprint(classification_report(y_test, vote_clf2.predict(X_test)))\n#0.86\n","metadata":{"execution":{"iopub.status.busy":"2022-08-04T09:19:58.485667Z","iopub.execute_input":"2022-08-04T09:19:58.486174Z","iopub.status.idle":"2022-08-04T09:20:19.521259Z","shell.execute_reply.started":"2022-08-04T09:19:58.486125Z","shell.execute_reply":"2022-08-04T09:20:19.519799Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"# 7. Submit the result","metadata":{}},{"cell_type":"code","source":"test = pandas.read_csv('/kaggle/input/titanic/test.csv')\n# pred_test = random_forest.predict(test)\nnp.sum(test.isna(),0)\n# Error: \"Fare\" has a missing value, in contrast to X_train and X_test.\n\nsurname = []\ntitle = []\nfor txt in test['Name']:\n      txt1 = txt.split(',')[0]\n      txt2 = (txt.split(\",\")[1]).split(\".\")[0]\n      surname.append(txt1)\n      title.append(txt2)\n\ntest['Surname'] = pandas.DataFrame(surname)\ntest['Title'] = pandas.DataFrame(title, dtype='category')\n\n## Adjust Title as for Train\ntitle_test = ((test.loc[:,'Title'].value_counts())[:6]).index.to_list()\n\nfor ind in test.index:\n    if not(test.loc[ind, 'Title'] in title_test):\n        test.loc[ind, 'Title'] = \"Others\"\n\ntest.loc[:,'Title'].value_counts()\n\ntest[\"Age\"] = test[\"Age\"].fillna(test.groupby(['Sex','Title'])['Age'].transform('mean'))\n# Not all missing values can be filled by group-by means; some might be missing.\n# test['Age'] = test['Age'].fillna(test['Age'].mean())\n\ntest['Fare'] = test['Fare'].fillna(test.groupby(['Pclass'], dropna=True)['Fare'].transform('mean'))\n\ntest2 = test.drop(['Name', 'PassengerId'], axis=1)\npred_test = vote_clf2.predict(test2)\n\nres1 = (test.copy())['PassengerId']\nresult = (pandas.DataFrame(res1)).assign(Survived = pred_test)\nresult.to_csv('submission.csv',index=False)\n\n## The end.","metadata":{"execution":{"iopub.status.busy":"2022-08-04T09:25:54.416742Z","iopub.execute_input":"2022-08-04T09:25:54.419060Z","iopub.status.idle":"2022-08-04T09:25:54.584441Z","shell.execute_reply.started":"2022-08-04T09:25:54.418995Z","shell.execute_reply":"2022-08-04T09:25:54.582790Z"},"trusted":true},"execution_count":42,"outputs":[]}]}